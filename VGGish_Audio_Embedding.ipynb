{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pathlib\n",
    "from tqdm import tqdm\n",
    "import F5signal as f5s\n",
    "from models.research.audioset.vggish import vggish_params\n",
    "from models.research.audioset.vggish import vggish_slim\n",
    "from models.research.audioset.vggish import vggish_input\n",
    "from models.research.audioset.vggish import vggish_postprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CreateVGGishNetwork(hop_size=0.96):   # Hop size is in seconds.\n",
    "  \"\"\"Define VGGish model, load the checkpoint, and return a dictionary that points\n",
    "  to the different tensors defined by the model.\n",
    "  \"\"\"\n",
    "  vggish_slim.define_vggish_slim()\n",
    "  checkpoint_path = 'vggish_model.ckpt'\n",
    "  vggish_params.EXAMPLE_HOP_SECONDS = hop_size\n",
    "  \n",
    "  vggish_slim.load_vggish_slim_checkpoint(sess, checkpoint_path)\n",
    "\n",
    "  features_tensor = sess.graph.get_tensor_by_name(\n",
    "      vggish_params.INPUT_TENSOR_NAME)\n",
    "  embedding_tensor = sess.graph.get_tensor_by_name(\n",
    "      vggish_params.OUTPUT_TENSOR_NAME)\n",
    "\n",
    "  layers = {'conv1': 'vggish/conv1/Relu',\n",
    "            'pool1': 'vggish/pool1/MaxPool',\n",
    "            'conv2': 'vggish/conv2/Relu',\n",
    "            'pool2': 'vggish/pool2/MaxPool',\n",
    "            'conv3': 'vggish/conv3/conv3_2/Relu',\n",
    "            'pool3': 'vggish/pool3/MaxPool',\n",
    "            'conv4': 'vggish/conv4/conv4_2/Relu',\n",
    "            'pool4': 'vggish/pool4/MaxPool',\n",
    "            'fc1': 'vggish/fc1/fc1_2/Relu',\n",
    "            'fc2': 'vggish/fc2/Relu',\n",
    "            'embedding': 'vggish/embedding',\n",
    "            'features': 'vggish/input_features',\n",
    "         }\n",
    "  g = tf.compat.v1.get_default_graph()\n",
    "  for k in layers:\n",
    "    layers[k] = g.get_tensor_by_name( layers[k] + ':0')\n",
    "    \n",
    "  return {'features': features_tensor,\n",
    "          'embedding': embedding_tensor,\n",
    "          'layers': layers,\n",
    "         }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ProcessWithVGGish(vgg, x, sr):\n",
    "  '''Run the VGGish model, starting with a sound (x) at sample rate\n",
    "  (sr). Return a whitened version of the embeddings. Sound must be scaled to be\n",
    "  floats between -1 and +1.'''\n",
    "\n",
    "  # Produce a batch of log mel spectrogram examples.\n",
    "  input_batch = vggish_input.waveform_to_examples(x, sr)\n",
    "  # print('Log Mel Spectrogram example: ', input_batch[0])\n",
    "\n",
    "  [embedding_batch] = sess.run([vgg['embedding']],\n",
    "                               feed_dict={vgg['features']: input_batch})\n",
    "\n",
    "  # Postprocess the results to produce whitened quantized embeddings.\n",
    "  pca_params_path = 'vggish_pca_params.npz'\n",
    "\n",
    "  pproc = vggish_postprocess.Postprocessor(pca_params_path)\n",
    "  postprocessed_batch = pproc.postprocess(embedding_batch)\n",
    "  # print('Postprocessed VGGish embedding: ', postprocessed_batch[0])\n",
    "  return postprocessed_batch[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def EmbeddingsFromVGGish(vgg, x, sr):\n",
    "  '''Run the VGGish model, starting with a sound (x) at sample rate\n",
    "  (sr). Return a dictionary of embeddings from the different layers\n",
    "  of the model.'''\n",
    "  # Produce a batch of log mel spectrogram examples.\n",
    "  input_batch = vggish_input.waveform_to_examples(x, sr)\n",
    "  # print('Log Mel Spectrogram example: ', input_batch[0])\n",
    "\n",
    "  layer_names = vgg['layers'].keys()\n",
    "  tensors = [vgg['layers'][k] for k in layer_names]\n",
    "  \n",
    "  results = sess.run(tensors,\n",
    "                     feed_dict={vgg['features']: input_batch})\n",
    "\n",
    "  resdict = {}\n",
    "  for i, k in enumerate(layer_names):\n",
    "    resdict[k] = results[i]\n",
    "    \n",
    "  return resdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from vggish_model.ckpt\n"
     ]
    }
   ],
   "source": [
    "import tensorflow  as tf\n",
    "tf.compat.v1.reset_default_graph()\n",
    "sess = tf.compat.v1.Session()\n",
    "\n",
    "vgg = CreateVGGishNetwork(0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = pathlib.Path(r'dev_data\\fan\\train')\n",
    "files = files.glob('*.wav')\n",
    "files=list(files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(columns=['name'] + ['y'] + ['id'] + list(range(0, 128)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "Empty DataFrame\nColumns: [name, y, id, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, ...]\nIndex: []\n\n[0 rows x 131 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>name</th>\n      <th>y</th>\n      <th>id</th>\n      <th>0</th>\n      <th>1</th>\n      <th>2</th>\n      <th>3</th>\n      <th>4</th>\n      <th>5</th>\n      <th>6</th>\n      <th>...</th>\n      <th>118</th>\n      <th>119</th>\n      <th>120</th>\n      <th>121</th>\n      <th>122</th>\n      <th>123</th>\n      <th>124</th>\n      <th>125</th>\n      <th>126</th>\n      <th>127</th>\n    </tr>\n  </thead>\n  <tbody>\n  </tbody>\n</table>\n<p>0 rows × 131 columns</p>\n</div>"
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3675/3675 [32:14<00:00,  1.90it/s]\n"
     ]
    }
   ],
   "source": [
    "for num,wav in tqdm(enumerate(files),total=len(files)):\n",
    "    name=wav.stem.split('_')[0]\n",
    "    id_wav=int(wav.stem.split('_')[2])\n",
    "    num_id_wav=int(wav.stem.split('_')[3])\n",
    "    wave = f5s.read_wave(str(wav))\n",
    "    x, sr = wave.ys, wave.framerate\n",
    "    postprocessed_batch = ProcessWithVGGish(vgg, x, sr)\n",
    "    if name=='normal':\n",
    "        l = [str(wav), 0, id_wav]\n",
    "        l.extend(postprocessed_batch)\n",
    "        df.loc[num] = l\n",
    "    else:\n",
    "        l = [str(wav), 1, id_wav]\n",
    "        l.extend(postprocessed_batch)\n",
    "        df.loc[num] = l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from config import TRAIN_WAV_DIR,DCASE_CSV_DIR_TRAIN\n",
    "df.to_csv(DCASE_CSV_DIR_TRAIN+'Xtrain.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "colab": {
   "name": "VGGish Audio Embedding Colab.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}